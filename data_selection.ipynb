{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7e55ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import DataStructs, Chem, RDLogger\n",
    "from rdkit.Chem import rdFingerprintGenerator\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection._split import _BaseKFold\n",
    "from typing import List\n",
    "from transformers import AutoTokenizer, PreTrainedModel, AutoModelForMaskedLM\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3807e421",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger=RDLogger.logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5442a947",
   "metadata": {},
   "outputs": [],
   "source": [
    "def EuclideanDist(pi, pj):\n",
    "  dv = np.array(pi)- np.array(pj)\n",
    "  return np.sqrt(dv * dv).sum()\n",
    "\n",
    "def ClusterData(data,nPts,distThresh,isDistData=False,distFunc=EuclideanDist):\n",
    "  \"\"\"  clusters the data points passed in and returns the list of clusters\n",
    "\n",
    "    **Arguments**\n",
    "\n",
    "      - data: a list of items with the input data \n",
    "        (see discussion of _isDistData_ argument for the exception)\n",
    "\n",
    "      - nPts: the number of points to be used\n",
    "\n",
    "      - distThresh: elements within this range of each other are considered\n",
    "        to be neighbors            \n",
    "\n",
    "      - isDistData: set this toggle when the data passed in is a\n",
    "          distance matrix.  The distance matrix should be stored\n",
    "          symmetrically. An example of how to do this:\n",
    "\n",
    "            dists = []\n",
    "            for i in range(nPts):\n",
    "              for j in range(i):\n",
    "                dists.append( distfunc(i,j) )\n",
    "\n",
    "      - distFunc: a function to calculate distances between points.\n",
    "          Receives 2 points as arguments, should return a float\n",
    "          \n",
    "    **Returns**\n",
    "\n",
    "      - a tuple of tuples containing information about the clusters:\n",
    "        ( (cluster1_elem1, cluster1_elem2, ...),\n",
    "          (cluster2_elem1, cluster2_elem2, ...),\n",
    "          ...\n",
    "        )  \n",
    "        The first element for each cluster is its centroid.\n",
    "\n",
    "  \"\"\"\n",
    "  if isDistData and len(data)>(nPts*(nPts-1)/2):\n",
    "    logger.warning(\"Distance matrix is too long\")\n",
    "  nbrLists = [None]*nPts\n",
    "  for i in range(nPts): nbrLists[i] = []\n",
    "\n",
    "  dmIdx=0\n",
    "  for i in range(nPts):\n",
    "    for j in range(i):\n",
    "      if not isDistData:\n",
    "        dij = distFunc(data[i],data[j])\n",
    "      else:\n",
    "        dij = data[dmIdx]\n",
    "        dmIdx+=1\n",
    "      if dij<=distThresh:\n",
    "        nbrLists[i].append(j)\n",
    "        nbrLists[j].append(i)\n",
    "  #print nbrLists\n",
    "  # sort by the number of neighbors:\n",
    "  tLists = [(len(y),x) for x,y in enumerate(nbrLists)]\n",
    "  tLists.sort()\n",
    "  tLists.reverse()\n",
    "\n",
    "  res = []\n",
    "  seen = [0]*nPts\n",
    "  while tLists:\n",
    "    nNbrs,idx = tLists.pop(0)\n",
    "    if seen[idx]:\n",
    "      continue\n",
    "    tRes = [idx]\n",
    "    for nbr in nbrLists[idx]:\n",
    "      if not seen[nbr]:\n",
    "        tRes.append(nbr)\n",
    "        seen[nbr]=1\n",
    "    res.append(tuple(tRes))\n",
    "  return tuple(res)\n",
    "\n",
    "def taylor_butina_clustering(fp_list: List[DataStructs.ExplicitBitVect], cutoff: float = 0.65) -> List[int]:\n",
    "  \"\"\"\n",
    "  Cluster a set of fingerprints using the RDKit Taylor-Butina implementation\n",
    "\n",
    "  Parameters:\n",
    "  ----------\n",
    "  fp_list: a list of fingerprints\n",
    "  cutoff: distance cutoff (1 - Tanimoto similarity)\n",
    "\n",
    "  Returns:\n",
    "  -------\n",
    "  A list of cluster ids\n",
    "  \"\"\"\n",
    "  dists = []\n",
    "  nfps = len(fp_list)\n",
    "  for i in range(1, nfps):\n",
    "      sims = DataStructs.BulkTanimotoSimilarity(fp_list[i], fp_list[:i])\n",
    "      dists.extend([1 - x for x in sims])\n",
    "  cluster_res = ClusterData(dists, nfps, cutoff, isDistData=True)\n",
    "  cluster_id_list = np.zeros(nfps, dtype=int)\n",
    "  for cluster_num, cluster in enumerate(cluster_res):\n",
    "      for member in cluster:\n",
    "          cluster_id_list[member] = cluster_num\n",
    "  return cluster_id_list.tolist()\n",
    "\n",
    "def get_butina_clusters(smiles_list: List[str], cutoff: float = 0.65) -> List[int]:\n",
    "  \"\"\"\n",
    "  Cluster a list of SMILES strings using the Butina clustering algorithm.\n",
    "\n",
    "  Parameters:\n",
    "  ----------\n",
    "  smiles_list: List of SMILES strings\n",
    "  cutoff: The cutoff value to use for clustering\n",
    "\n",
    "  Returns:\n",
    "  -------\n",
    "  List of cluster labels corresponding to each SMILES string in the input list.\n",
    "  \"\"\"\n",
    "  mol_list = [Chem.MolFromSmiles(x) for x in smiles_list]\n",
    "  fg = rdFingerprintGenerator.GetMorganGenerator(radius=2, fpSize=1024)\n",
    "  fp_list = [fg.GetFingerprint(x) for x in mol_list]\n",
    "  return taylor_butina_clustering(fp_list, cutoff=cutoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ee7bc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for the MLM-finetuned model\n",
    "class MoLFormerMLMWithRegressionHead(PreTrainedModel):\n",
    "    def __init__(self, pretrained_model, config=None):\n",
    "        if config is None:\n",
    "            config = pretrained_model.config\n",
    "        super().__init__(config)\n",
    "        self.backbone = pretrained_model\n",
    "        hidden_size = self.backbone.config.hidden_size\n",
    "        self.regression_head = nn.Linear(hidden_size, 1)\n",
    "        self.config = config\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        outputs = self.backbone(input_ids=input_ids, \n",
    "                               attention_mask=attention_mask,\n",
    "                               output_hidden_states=True)  # MLM needs hidden states explicitly\n",
    "        last_hidden_state = outputs.hidden_states[-1]  # Access last layerâ€™s hidden states\n",
    "        cls_hidden_state = last_hidden_state[:, 0, :]\n",
    "        output = self.regression_head(cls_hidden_state)\n",
    "        return output.squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9522a338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Variables\n",
    "\n",
    "EXTERNAL_DATA_LOCATION = '/home/sunag/Documents/NNTI/Project/task2/new_train_data.tsv'\n",
    "ext_data = pd.read_csv(EXTERNAL_DATA_LOCATION, sep='\\t')\n",
    "ext_data.rename({'Label' : 'label'}, inplace = True, axis = 1)\n",
    "\n",
    "# Merge the two datasets\n",
    "data = ext_data\n",
    "\n",
    "data[\"ButinaCluster\"] = get_butina_clusters(data[\"SMILES\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5364bbce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>ButinaCluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4242.000000</td>\n",
       "      <td>4242.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.187506</td>\n",
       "      <td>205.008015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.198735</td>\n",
       "      <td>262.898946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.420000</td>\n",
       "      <td>23.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.360000</td>\n",
       "      <td>83.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.100000</td>\n",
       "      <td>284.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.500000</td>\n",
       "      <td>1056.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             label  ButinaCluster\n",
       "count  4242.000000    4242.000000\n",
       "mean      2.187506     205.008015\n",
       "std       1.198735     262.898946\n",
       "min      -1.500000       0.000000\n",
       "25%       1.420000      23.000000\n",
       "50%       2.360000      83.000000\n",
       "75%       3.100000     284.000000\n",
       "max       4.500000    1056.000000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c9cd1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupKFoldShuffleTV(_BaseKFold):\n",
    "    \"\"\"\n",
    "    Group-aware K-fold cross-validator with optional shuffling,\n",
    "    providing train, validation, and test indices.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    n_splits : int, default=5\n",
    "        Number of folds. Must be at least 3 (train, validation, test).\n",
    "\n",
    "    shuffle : bool, default=False\n",
    "        Whether to shuffle the groups before splitting into folds.\n",
    "\n",
    "    random_state : int, RandomState instance or None, default=None\n",
    "        Controls the randomness of the shuffling. Pass an int for reproducible\n",
    "        output across multiple function calls. Ignored if `shuffle=False`.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_splits=5, *, shuffle=False, random_state=None):\n",
    "        if n_splits < 3:\n",
    "            raise ValueError(\"n_splits must be at least 3 to provide train, validation, and test splits.\")\n",
    "        super().__init__(n_splits=n_splits, shuffle=shuffle, random_state=random_state)\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        if groups is None:\n",
    "            raise ValueError(\"Groups must be provided for group-aware splitting.\")\n",
    "\n",
    "        unique_groups = np.unique(groups)\n",
    "        \n",
    "        if self.shuffle:\n",
    "            rng = np.random.RandomState(self.random_state)\n",
    "            rng.shuffle(unique_groups)\n",
    "        \n",
    "        split_groups = np.array_split(unique_groups, self.n_splits)\n",
    "        \n",
    "        for i in range(self.n_splits):\n",
    "            test_group_ids = split_groups[i]\n",
    "            val_group_ids = split_groups[(i + 1) % self.n_splits]  # Next fold for validation\n",
    "            \n",
    "            test_mask = np.isin(groups, test_group_ids)\n",
    "            val_mask = np.isin(groups, val_group_ids)\n",
    "            train_mask = ~(test_mask | val_mask)\n",
    "            \n",
    "            train_idx = np.where(train_mask)[0]\n",
    "            val_idx = np.where(val_mask)[0]\n",
    "            test_idx = np.where(test_mask)[0]\n",
    "            \n",
    "            yield train_idx, val_idx, test_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f2a8a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dict = {}\n",
    "\n",
    "for fold, (train_idx, val_idx, test_idx) in enumerate(GroupKFoldShuffleTV(n_splits=5, shuffle=True, random_state=42).split(X=data, groups=data[\"ButinaCluster\"])):\n",
    "    split_dict['train'] = train_idx\n",
    "    split_dict['validation'] = val_idx\n",
    "    split_dict['test'] = test_idx\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee853c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.smiles = data['SMILES'].values\n",
    "        self.label = data['Label'].values\n",
    "    \n",
    "    def __len__(self):\n",
    "        return(len(self.smiles))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        smile = self.smiles[idx]\n",
    "        value = self.label[idx]\n",
    "        \n",
    "        return smile, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a213ae48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    \n",
    "    def __init__(self, data: pd.DataFrame, fin_model: nn.Module, split_dict: dict, **kwargs):\n",
    "        \"\"\"\n",
    "        Trainer initialization\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        data: pd.DataFrame\n",
    "            col ['SMILES', 'label']\n",
    "        \n",
    "        fin_model: nn.Module\n",
    "            - The neural network to be trained in this instance\n",
    "\n",
    "        split_dict: dict\n",
    "            - A dictionary of the indices of train, test and validation datasets\n",
    "            \n",
    "        **kwargs: Optional\n",
    "            - model_type (str): ['nofit', 'finefit']. Default is 'nofit'\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        print('Data initialization')\n",
    "        self.train_idx = split_dict['train']\n",
    "        self.valid_idx = split_dict['validation']\n",
    "        self.test_idx = split_dict['test']\n",
    "        # Create training, test and validation datasets\n",
    "        print(f'Train Points: {len(self.train_idx)}')\n",
    "        print(f'Valid Points: {len(self.valid_idx)}')\n",
    "        print(f'Test_Points: {len(self.test_idx)}')\n",
    "\n",
    "        self.train_df = Dataset(data.iloc[self.train_idx])\n",
    "        self.valid_df = Dataset(data.iloc[self.valid_idx])\n",
    "        self.test_df  = Dataset(data.iloc[self.test_idx])\n",
    "        \n",
    "        \n",
    "        print('Model Initialization')\n",
    "        MODEL_NAME = \"ibm/MoLFormer-XL-both-10pct\"\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.pre_token = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "        self.losses = {'Train': [], 'Valid': []}\n",
    "        self.model = fin_model.to(self.device)\n",
    "        print('Model Initialization successful')\n",
    "        \n",
    "    def train(self, train_params: dict):\n",
    "        \"\"\"\n",
    "        Training\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        train_params: dict\n",
    "            - epochs (int): No. of epochs\n",
    "            - lr (float) : Learning Rate\n",
    "            - wt_decay (float) : Weight Decay\n",
    "            - batch_size (int) : Batch Size\n",
    "        \"\"\"\n",
    "        \n",
    "        print('Training Starting...')\n",
    "        epochs = train_params['epochs']\n",
    "        lr = train_params['lr']\n",
    "        wt_decay = train_params['wt_decay']\n",
    "        batch_size = train_params['batch_size']\n",
    "\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=lr, weight_decay=wt_decay)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        \n",
    "        self.train_loader = DataLoader(self.train_df, batch_size = batch_size)\n",
    "        self.valid_loader = DataLoader(self.valid_df, batch_size = batch_size)\n",
    "        self.test_loader = DataLoader(self.test_df, batch_size = batch_size)\n",
    "        \n",
    "        for i in range(epochs):\n",
    "            \n",
    "            # Train\n",
    "            self.model.train()\n",
    "            train_loss = 0.0\n",
    "            with tqdm(total=len(self.train_loader)) as pbar:\n",
    "                pbar.set_description(f'Epoch: {i} - Train')\n",
    "                \n",
    "                for smiles, values in self.train_loader:\n",
    "                    values = torch.tensor(values , dtype = torch.float, device = self.device)\n",
    "                    smiles = self.pre_token(smiles, padding = True, return_tensors = 'pt').to(self.device)\n",
    "                    #print(smiles)\n",
    "                    \n",
    "                    self.optimizer.zero_grad()\n",
    "                    predict = self.model(**smiles)\n",
    "                    loss = self.criterion(predict, values)\n",
    "                    loss.backward()\n",
    "                    self.optimizer.step()\n",
    "                    \n",
    "                    train_loss += loss\n",
    "                    pbar.update(1)\n",
    "            self.losses['Train'].append(float(train_loss/len(self.train_loader)))\n",
    "            \n",
    "            # Valid\n",
    "            self.model.eval()\n",
    "            valid_loss = 0.0\n",
    "            with tqdm(total=len(self.valid_loader)) as pbar:\n",
    "                pbar.set_description(f'Epoch: {i} - Valid')\n",
    "                \n",
    "                for smiles, values in self.valid_loader:\n",
    "                    values = torch.tensor(values , dtype = torch.float, device = self.device)\n",
    "                    smiles = self.pre_token(smiles, padding = True, return_tensors = 'pt').to(self.device)\n",
    "                    \n",
    "                    self.optimizer.zero_grad()\n",
    "                    predict = self.model(**smiles)\n",
    "                    loss = self.criterion(predict, values)\n",
    "                    loss.backward()\n",
    "                    self.optimizer.step()\n",
    "                    \n",
    "                    valid_loss += loss\n",
    "                    pbar.update(1)\n",
    "            self.losses['Valid'].append(float(valid_loss/len(self.valid_loader)))\n",
    "            \n",
    "            \n",
    "    def test(self):\n",
    "        print('Testing Starting...')\n",
    "        \n",
    "        test_loss, ss_total, ss_residual = 0.0, 0.0, 0.0\n",
    "        with tqdm(total=len(self.test_loader)) as pbar:\n",
    "            pbar.set_description('Testing')\n",
    "            self.pred_results = []\n",
    "            self.true_results = []\n",
    "            \n",
    "            for smiles, values in self.test_loader:\n",
    "                values = torch.tensor(values , dtype = torch.float, device = self.device)\n",
    "                smiles = self.pre_token(smiles, padding = True, return_tensors = 'pt').to(self.device)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    predict = self.model(**smiles)\n",
    "                    loss = self.criterion(predict , values)\n",
    "\n",
    "                    \n",
    "                    # Prediction vs True values\n",
    "                    self.pred_results.extend(predict.cpu().numpy().tolist())\n",
    "                    self.true_results.extend(values.cpu().numpy().tolist())\n",
    "                    \n",
    "                # Loss for a batch\n",
    "                test_loss += loss.item()\n",
    "                ss_total += torch.sum((values - values.mean()) ** 2).item()\n",
    "                ss_residual += torch.sum((predict - values) ** 2).item()\n",
    "                pbar.update(1)\n",
    "                \n",
    "        # Avg loss over all batches this epoch\n",
    "        self.avg_loss = float(test_loss / len(self.test_loader))\n",
    "        r2_score = 1 - (ss_residual / ss_total) if ss_total > 0 else float('-inf')\n",
    "        print(f\"Test Error: \\n RÂ² Score: {r2_score:.4f}\")\n",
    "        \n",
    "        \n",
    "        # Doing a test with train dataset\n",
    "        with tqdm(total=len(self.train_loader)) as pbar:\n",
    "            pbar.set_description('Testing with Train Dataset')\n",
    "            self.Trainpred_results = []\n",
    "            self.Traintrue_results = []\n",
    "            \n",
    "            for smiles, values in self.train_loader:\n",
    "                values = torch.tensor(values , dtype = torch.float, device = self.device)\n",
    "                smiles = self.pre_token(smiles, padding = True, return_tensors = 'pt').to(self.device)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    predict = self.model(**smiles)\n",
    "                    loss = self.criterion(predict , values)\n",
    "\n",
    "                    \n",
    "                    # Prediction vs True values\n",
    "                    self.Trainpred_results.extend(predict.cpu().numpy().tolist())\n",
    "                    self.Traintrue_results.extend(values.cpu().numpy().tolist())\n",
    "                    \n",
    "                pbar.update(1)\n",
    "    \n",
    "    def save(self, save_path):\n",
    "        \n",
    "        path = Path(save_path)\n",
    "        path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Saving Losses\n",
    "        self.loss_df = pd.DataFrame(self.losses)\n",
    "        self.loss_df.to_csv(f'{save_path}/losses.csv', index = False)\n",
    "        \n",
    "        # Train Loss Plot\n",
    "        fig , ax = plt.subplots(1,1)\n",
    "        ax.plot(range(1, len(self.loss_df['Train'])+1) , self.loss_df['Train'])\n",
    "        ax.set_xlabel('Epochs')\n",
    "        ax.set_ylabel('Train Loss')\n",
    "        ax.grid(True)\n",
    "        fig.savefig(f'{save_path}/TrainLoss.png' , dpi = 300)\n",
    "        plt.close()\n",
    "        \n",
    "        # Valid Loss Plot\n",
    "        fig , ax = plt.subplots(1,1)\n",
    "        ax.plot(range(1, len(self.loss_df['Valid'])+1) , self.loss_df['Valid'])\n",
    "        ax.set_xlabel('Epochs')\n",
    "        ax.set_ylabel('Valid Loss')\n",
    "        ax.grid(True)\n",
    "        fig.savefig(f'{save_path}/ValidLoss.png' , dpi = 300)\n",
    "        plt.close()\n",
    "        \n",
    "        # Test prediction\n",
    "        fig , ax = plt.subplots(1,1)\n",
    "        hb = ax.hexbin(x = self.true_results, y = self.pred_results, cmap = 'inferno', gridsize = 50)\n",
    "        cb = fig.colorbar(hb, ax=ax)\n",
    "        cb.set_label(\"Loss Value Range\")\n",
    "        ax.set_title(f'Average Test Loss: {self.avg_loss}')\n",
    "        ax.set_xlabel('True values')\n",
    "        ax.set_ylabel('Predicted values')\n",
    "        ax.grid(True)\n",
    "        \n",
    "        fig.savefig(f'{save_path}/Test_TrueVpred.png' , dpi = 300)\n",
    "        plt.close()\n",
    "        \n",
    "        # Train prediction\n",
    "        fig , ax = plt.subplots(1,1)\n",
    "        hb = ax.hexbin(x = self.Traintrue_results, y = self.Trainpred_results, cmap = 'inferno', gridsize = 50)\n",
    "        cb = fig.colorbar(hb, ax=ax)\n",
    "        cb.set_label(\"Counts\")\n",
    "        ax.set_xlabel('True values')\n",
    "        ax.set_ylabel('Predicted values')\n",
    "        ax.grid(True)\n",
    "        \n",
    "        fig.savefig(f'{save_path}/Train_TrueVpred.png' , dpi = 300)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e0e4441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train parameters\n",
    "train_params = {\n",
    "    'epochs'    : 1,\n",
    "    'lr'        : 0.0001,\n",
    "    'wt_decay'  : 1e-5,\n",
    "    'batch_size': 128\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "89b3d47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '/home/sunag/Documents/NNTI/Project/task3/molformerMLM'\n",
    "pre_model = AutoModelForMaskedLM.from_pretrained(PATH, trust_remote_code=True)\n",
    "fin_model = MoLFormerMLMWithRegressionHead(pre_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15aad849",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Trainer(data, fin_model, split_dict)\n",
    "x.train(train_params)\n",
    "x.test()\n",
    "x.save('DS/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nnti_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
