{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, PreTrainedModel, AutoModelForMaskedLM\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for the MLM-finetuned model\n",
    "class MoLFormerMLMWithRegressionHead(PreTrainedModel):\n",
    "    def __init__(self, pretrained_model, config=None):\n",
    "        if config is None:\n",
    "            config = pretrained_model.config\n",
    "        super().__init__(config)\n",
    "        self.backbone = pretrained_model\n",
    "        hidden_size = self.backbone.config.hidden_size\n",
    "        self.regression_head = nn.Linear(hidden_size, 1)\n",
    "        self.config = config\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        outputs = self.backbone(input_ids=input_ids, \n",
    "                               attention_mask=attention_mask,\n",
    "                               output_hidden_states=True)  # MLM needs hidden states explicitly\n",
    "        last_hidden_state = outputs.hidden_states[-1]  # Access last layerâ€™s hidden states\n",
    "        cls_hidden_state = last_hidden_state[:, 0, :]\n",
    "        output = self.regression_head(cls_hidden_state)\n",
    "        return output.squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.smiles = data['SMILES'].values\n",
    "        self.label = data['label'].values\n",
    "    \n",
    "    def __len__(self):\n",
    "        return(len(self.smiles))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        smile = self.smiles[idx]\n",
    "        value = self.label[idx]\n",
    "        \n",
    "        return smile, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Collate function for the DataLoader\n",
    "    \"\"\"\n",
    "    smile, value = zip(*batch)\n",
    "    return list(smile), list(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    \n",
    "    def __init__(self, data: pd.DataFrame, model_path: str, **kwargs):\n",
    "        \"\"\"\n",
    "        Trainer initialization\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        data: pd.DataFrame\n",
    "            col ['SMILES', 'label']\n",
    "        \n",
    "        model_path: str\n",
    "            Path to the model\n",
    "            \n",
    "        **kwargs: Optional\n",
    "            - model_type (str): ['nofit', 'finefit']. Default is 'nofit'\n",
    "            \n",
    "        \"\"\"\n",
    "        # Default values\n",
    "        self.default_kwargs = {'model_type' : 'nofit'}\n",
    "        self.default_kwargs.update(kwargs)\n",
    "        \n",
    "        # Update default values if input\n",
    "        for key, value in self.default_kwargs.items():\n",
    "            setattr(self, key, value)\n",
    "        \n",
    "        \n",
    "        print('Data initialization')\n",
    "        # Generating Train, Valid, Test dataset\n",
    "        train_len = int(len(data)*0.7)\n",
    "        valid_len = int(len(data)*0.2)\n",
    "        test_len = int(len(data)*0.1)\n",
    "        print(f'Train Points: {train_len}')\n",
    "        print(f'Valid Points: {valid_len}')\n",
    "        print(f'Test_Points: {test_len}')\n",
    "\n",
    "        self.train_df = Dataset(data.iloc[0:train_len, :])\n",
    "        self.valid_df = Dataset(data.iloc[train_len:train_len+valid_len, :])\n",
    "        self.test_df  = Dataset(data.iloc[train_len+valid_len:, :])\n",
    "        \n",
    "        \n",
    "        print('Model Initialization')\n",
    "        MODEL_NAME = \"ibm/MoLFormer-XL-both-10pct\"\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.pre_model = AutoModelForMaskedLM.from_pretrained(model_path, trust_remote_code=True)\n",
    "        self.pre_token = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "        self.losses = {'Train': [], 'Valid': []}\n",
    "        \n",
    "        match self.model_type:\n",
    "            case 'nofit':\n",
    "                self.model = self._nofit(self.pre_model).to(self.device) # MolFormer + RegHead - FineTuning\n",
    "            case 'finefit':\n",
    "                self.model = self._bitfit(self.pre_model).to(self.device) # MolFormer + RegHead + FineTuning\n",
    "        \n",
    "    def train(self, train_params: dict):\n",
    "        \"\"\"\n",
    "        Training\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        train_params: dict\n",
    "            - epochs (int): No. of epochs\n",
    "            - lr (float) : Learning Rate\n",
    "            - wt_decay (float) : Weight Decay\n",
    "            - batch_size (int) : Batch Size\n",
    "        \"\"\"\n",
    "        \n",
    "        print('Training Starting...')\n",
    "        epochs = train_params['epochs']\n",
    "        lr = train_params['lr']\n",
    "        wt_decay = train_params['wt_decay']\n",
    "        batch_size = train_params['batch_size']\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=lr, weight_decay=wt_decay)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        \n",
    "        self.train_loader = DataLoader(self.train_df, batch_size = batch_size)\n",
    "        self.valid_loader = DataLoader(self.valid_df, batch_size = batch_size)\n",
    "        self.test_loader = DataLoader(self.test_df, batch_size = batch_size)\n",
    "        \n",
    "        for i in range(epochs):\n",
    "            \n",
    "            # Train\n",
    "            self.model.train()\n",
    "            train_loss = 0.0\n",
    "            with tqdm(total=len(self.train_loader)) as pbar:\n",
    "                pbar.set_description(f'Epoch: {i} - Train')\n",
    "                \n",
    "                for smiles, values in self.train_loader:\n",
    "                    values = torch.tensor(values , dtype = torch.float, device = self.device)\n",
    "                    smiles = self.pre_token(smiles, padding = True, return_tensors = 'pt').to(self.device)\n",
    "                    #print(smiles)\n",
    "                    \n",
    "                    self.optimizer.zero_grad()\n",
    "                    predict = self.model(**smiles)\n",
    "                    loss = self.criterion(predict, values)\n",
    "                    loss.backward()\n",
    "                    self.optimizer.step()\n",
    "                    \n",
    "                    train_loss += loss\n",
    "                    pbar.update(1)\n",
    "            self.losses['Train'].append(float(train_loss/len(self.train_loader)))\n",
    "            \n",
    "            \n",
    "            # Valid\n",
    "            self.model.eval()\n",
    "            valid_loss = 0.0\n",
    "            with tqdm(total=len(self.valid_loader)) as pbar:\n",
    "                pbar.set_description(f'Epoch: {i} - Valid')\n",
    "                \n",
    "                for smiles, values in self.valid_loader:\n",
    "                    values = torch.tensor(values , dtype = torch.float, device = self.device)\n",
    "                    smiles = self.pre_token(smiles, padding = True, return_tensors = 'pt').to(self.device)\n",
    "                    \n",
    "                    self.optimizer.zero_grad()\n",
    "                    predict = self.model(**smiles)\n",
    "                    loss = self.criterion(predict, values)\n",
    "                    loss.backward()\n",
    "                    self.optimizer.step()\n",
    "                    \n",
    "                    valid_loss += loss\n",
    "                    pbar.update(1)\n",
    "            self.losses['Valid'].append(float(valid_loss/len(self.valid_loader)))\n",
    "            \n",
    "    def test(self):\n",
    "        print('Testing Starting...')\n",
    "        \n",
    "        test_loss = 0.0\n",
    "        with tqdm(total=len(self.test_loader)) as pbar:\n",
    "            pbar.set_description('Testing')\n",
    "            self.pred_results = []\n",
    "            self.true_results = []\n",
    "            \n",
    "            for smiles, values in self.test_loader:\n",
    "                values = torch.tensor(values , dtype = torch.float, device = self.device)\n",
    "                smiles = self.pre_token(smiles, padding = True, return_tensors = 'pt').to(self.device)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    predict = self.model(**smiles)\n",
    "                    loss = self.criterion(predict , values)\n",
    "\n",
    "                    \n",
    "                    # Prediction vs True values\n",
    "                    self.pred_results.extend(predict.cpu().numpy().tolist())\n",
    "                    self.true_results.extend(values.cpu().numpy().tolist())\n",
    "                    \n",
    "                # Loss for a batch\n",
    "                test_loss += loss.item()\n",
    "                pbar.update(1)\n",
    "                \n",
    "        # Avg loss over all batches this epoch\n",
    "        self.avg_loss = float(test_loss / len(self.test_loader))\n",
    "\n",
    "        \n",
    "        \n",
    "        # Doing a test with train dataset\n",
    "        with tqdm(total=len(self.train_loader)) as pbar:\n",
    "            pbar.set_description('Testing with Train Dataset')\n",
    "            self.Trainpred_results = []\n",
    "            self.Traintrue_results = []\n",
    "            \n",
    "            for smiles, values in self.train_loader:\n",
    "                values = torch.tensor(values , dtype = torch.float, device = self.device)\n",
    "                smiles = self.pre_token(smiles, padding = True, return_tensors = 'pt').to(self.device)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    predict = self.model(**smiles)\n",
    "                    loss = self.criterion(predict , values)\n",
    "\n",
    "                    \n",
    "                    # Prediction vs True values\n",
    "                    self.Trainpred_results.extend(predict.cpu().numpy().tolist())\n",
    "                    self.Traintrue_results.extend(values.cpu().numpy().tolist())\n",
    "                    \n",
    "                pbar.update(1)\n",
    "    \n",
    "    def save(self, save_path):\n",
    "        \n",
    "        path = Path(save_path)\n",
    "        path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Saving Losses\n",
    "        self.loss_df = pd.DataFrame(self.losses)\n",
    "        self.loss_df.to_csv(f'{save_path}/losses.csv', index = False)\n",
    "        \n",
    "        # Train Loss Plot\n",
    "        fig , ax = plt.subplots(1,1)\n",
    "        ax.plot(range(1, len(self.loss_df['Train'])+1) , self.loss_df['Train'])\n",
    "        ax.set_xlabel('Epochs')\n",
    "        ax.set_ylabel('Train Loss')\n",
    "        ax.grid(True)\n",
    "        fig.savefig(f'{save_path}/TrainLoss.png' , dpi = 300)\n",
    "        plt.close()\n",
    "        \n",
    "        # Valid Loss Plot\n",
    "        fig , ax = plt.subplots(1,1)\n",
    "        ax.plot(range(1, len(self.loss_df['Valid'])+1) , self.loss_df['Valid'])\n",
    "        ax.set_xlabel('Epochs')\n",
    "        ax.set_ylabel('Valid Loss')\n",
    "        ax.grid(True)\n",
    "        fig.savefig(f'{save_path}/ValidLoss.png' , dpi = 300)\n",
    "        plt.close()\n",
    "        \n",
    "        # Test prediction\n",
    "        fig , ax = plt.subplots(1,1)\n",
    "        hb = ax.hexbin(x = self.true_results, y = self.pred_results, cmap = 'inferno', gridsize = 50)\n",
    "        cb = fig.colorbar(hb, ax=ax)\n",
    "        cb.set_label(\"Loss Value Range\")\n",
    "        ax.set_title(f'Average Test Loss: {self.avg_loss}')\n",
    "        ax.set_xlabel('True values')\n",
    "        ax.set_ylabel('Predicted values')\n",
    "        ax.grid(True)\n",
    "        \n",
    "        fig.savefig(f'{save_path}/Test_TrueVpred.png' , dpi = 300)\n",
    "        plt.close()\n",
    "        \n",
    "        # Train prediction\n",
    "        fig , ax = plt.subplots(1,1)\n",
    "        ax.hexbin(x = self.Traintrue_results, y = self.Trainpred_results, cmap = 'inferno', gridsize = 50)\n",
    "        ax.set_xlabel('True values')\n",
    "        ax.set_ylabel('Predicted values')\n",
    "        ax.grid(True)\n",
    "        \n",
    "        fig.savefig(f'{save_path}/Train_TrueVpred.png' , dpi = 300)\n",
    "        plt.close()     \n",
    "        \n",
    "    def _bitfit(self,model):\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                param.requires_grad = True\n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "                \n",
    "        fin_model = MoLFormerMLMWithRegressionHead(model)\n",
    "        return fin_model\n",
    "    \n",
    "    def _nofit(self,model):\n",
    "        for _, param in model.named_parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        fin_model = MoLFormerMLMWithRegressionHead(model)\n",
    "        return fin_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Variables\n",
    "\n",
    "# Add the path to the external dataset here\n",
    "data1 = pd.read_csv(\"\")\n",
    "data1 = data1[[\"SMILES\", \"Label\"]].dropna()\n",
    "data1.rename(columns = {'Label':'label'}, inplace = True)\n",
    "\n",
    "# Path to huggingFace dataset\n",
    "DATASET_PATH = \"scikit-fingerprints/MoleculeNet_Lipophilicity\"\n",
    "\n",
    "# load the dataset from HuggingFace\n",
    "dataset = load_dataset(DATASET_PATH)\n",
    "hf_df = pd.DataFrame(dataset['train'])\n",
    "hf_df = hf_df[[\"SMILES\", \"label\"]].dropna()\n",
    "\n",
    "# Merge the two datasets\n",
    "data = pd.concat([data1, hf_df], axis = 0).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train parameters\n",
    "train_params = {\n",
    "    'epochs'    : 50,\n",
    "    'lr'        : 0.0001,\n",
    "    'wt_decay'  : 1e-5,\n",
    "    'batch_size': 128\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the path to the pre-trained MLM model here\n",
    "PATH = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Trainer(data, model_type = 'nofit', model_path = PATH)\n",
    "x.train(train_params)\n",
    "x.test()\n",
    "x.save('noFit/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Trainer(data, model_type = 'finefit', model_path = PATH)\n",
    "x.train(train_params)\n",
    "x.test()\n",
    "x.save('fineFit/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nnti_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
